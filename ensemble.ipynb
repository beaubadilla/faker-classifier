{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TODO\n",
    "- EDA\n",
    "- error analysis\n",
    "    - look at the type1 vs type2 errors\n",
    "    - manually try to interpret what is it failing to see\n",
    "- find ways to reduce dimensionality\n",
    "    - unsupervised learning\n",
    "    - feature engineering\n",
    "        - PCA\n",
    "        - umap\n",
    "- mine more data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys  \n",
    "print(sys.executable)\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "from platform import system\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Get file path\n",
    "if system() == \"Windows\":\n",
    "    filepath = \"C:\\\\Users\\\\Beau\\\\Desktop\\\\ML\\\\faker\\\\merged9.csv\"\n",
    "# elif system() == \"Linux\":\n",
    "# filepath = \"\"\n",
    "else:\n",
    "    print(f\"Unfamiliar OS. Cannot set file path to csv file.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load csv\n",
    "print(f'Loading \"{filepath}\"')\n",
    "dataframe = pandas.read_csv(filepath)\n",
    "print(f\"Finish loading.\")\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IF YOU WANT TO BALANCE THE TARGET\n",
    "# num_wins = dataframe['W/L'].value_counts()[0]\n",
    "# num_losses = dataframe['W/L'].value_counts()[1]\n",
    "# print(dataframe['W/L'].value_counts())\n",
    "\n",
    "# X = dataframe[dataframe['W/L'] == \"Win\"].sample(num_losses, random_state=0)\n",
    "# dataframe = pandas.concat([X, dataframe[dataframe['W/L'] == \"Loss\"]])\n",
    "# print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_use = ['Side', 'tournament_curr_win_percentage', 'teams_region', 'teammate_top_champion','teammate_jungle_champion','teammate_mid_champion','teammate_adc_champion','teammate_support_champion','opponent_top_champion','opponent_jungle_champion','opponent_mid_champion','opponent_adc_champion','opponent_support_champion']\n",
    "cols_to_use = ['Side', 'tournament_curr_win_percentage', 'teams_region', 'teammate_role_top','teammate_role_jungle','teammate_role_mid','teammate_role_adc','teammate_role_support','enemy_role_top','enemy_role_jungle','enemy_role_mid','enemy_role_adc','enemy_role_support']\n",
    "\n",
    "X = dataframe[cols_to_use]\n",
    "\n",
    "y = dataframe[\"W/L\"]\n",
    "def transform_game_result(game_result):\n",
    "    if game_result.lower() == \"win\":\n",
    "        return 1\n",
    "    elif game_result.lower() == \"loss\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(\"\\\"W/L\\\" column has invalid values\")\n",
    "y = y.transform(transform_game_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate dataset in train, cv, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Get categorical cols\n",
    "categorical_cols = [col for col in X_train.columns\n",
    "                    if X_train[col].dtype == \"object\"]\n",
    "print(f\"categorical_cols = {categorical_cols}\\n\")\n",
    "# Get numerical cols\n",
    "numerical_cols = [col for col in X_train.columns\n",
    "                  if X_train[col].dtype in [\"int64\", \"float64\"]]\n",
    "print(f\"numerical_cols = {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before the pipeline, the shape of X_train is {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish baseline performance\n",
    "BASELINE_PERFORMANCE = 0.65  # taken from baseline_performance.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "xgb_model = XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "under_sampler = RandomUnderSampler(random_state=0)\n",
    "rf_model = make_pipeline(under_sampler, rf_model)\n",
    "xgb_model = make_pipeline(under_sampler, xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_nn_model():\n",
    "#     nn_model = keras.Sequential([\n",
    "#         layers.Dense(512, activation='relu', input_shape=[559]),\n",
    "#         layers.Dropout(rate=0.5),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Dense(512, activation='relu'),\n",
    "#         layers.Dropout(rate=0.5),\n",
    "#         layers.BatchNormalization(),\n",
    "#         # layers.Dense(512, activation='relu'),\n",
    "#         # layers.Dropout(rate=0.1),\n",
    "#         # layers.BatchNormalization(),\n",
    "#         # layers.Dense(512, activation='relu'),\n",
    "#         # layers.Dropout(rate=0.1),\n",
    "#         # layers.BatchNormalization(),\n",
    "#         layers.Dense(1, activation='sigmoid'),\n",
    "#     ])\n",
    "#     nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "#     return nn_model\n",
    "# early_stopping = keras.callbacks.EarlyStopping(\n",
    "#     patience=30,\n",
    "#     min_delta=0.001,\n",
    "#     restore_best_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_model = KerasClassifier(build_fn=create_nn_model, verbose=0)\n",
    "# nn_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', nn_model)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vscode the window has crashed reason oom 536870904\n",
    "# rf_params = [{\n",
    "#     \"model__n_estimators\": [25, 50, 75],\n",
    "#     \"model__max_depth\": [5, 10, 15, 20, 25, 30],\n",
    "#     \"model__max_samples\": [0.2, 0.4, 0.6, 0.8],\n",
    "\n",
    "#     # \"max_leaf_nodes\": []\n",
    "# }]\n",
    "# IF RF_MODEL IS A PIPELINE, NEED TO ADD PREFIXES\n",
    "rf_params = [{\n",
    "    \"model__randomforestclassifier__n_estimators\": [15, 25, 35],\n",
    "    \"model__randomforestclassifier__max_depth\": [3, 5, 8],\n",
    "    \"model__randomforestclassifier__max_samples\": [0.5, 0.6, 0.7],\n",
    "\n",
    "    # \"max_leaf_nodes\": []\n",
    "}]\n",
    "\n",
    "# IF XGB_MODEL IS A PIPELINE, NEED TO ADD PREFIXES\n",
    "xgb_params = [{\n",
    "    'model__xgbclassifier__alpha': [1],\n",
    "    'model__xgbclassifier__lambda': [1],\n",
    "    'model__xgbclassifier__learning_rate': [0.5],\n",
    "    'model__xgbclassifier__max_delta_step': [3],\n",
    "    'model__xgbclassifier__max_depth': [5],\n",
    "    'model__xgbclassifier__min_child_weight': [10],\n",
    "    'model__xgbclassifier__min_split_loss': [1],\n",
    "    'model__xgbclassifier__subsample': [0.5]\n",
    "}]\n",
    "# xgb_params = [{\n",
    "#     'model__alpha': [1],\n",
    "#     'model__lambda': [1],\n",
    "#     'model__learning_rate': [0.5],\n",
    "#     'model__max_delta_step': [3],\n",
    "#     'model__max_depth': [5],\n",
    "#     'model__min_child_weight': [10],\n",
    "#     'model__min_split_loss': [1],\n",
    "#     'model__subsample': [0.5]\n",
    "# }]\n",
    "# ~10 mins\n",
    "# xgb_params = [{\n",
    "#     # Prevents overfitting\n",
    "#     \"model__learning_rate\": [0.5, 0.7],\n",
    "#     \"model__max_depth\": [5, 10,15],\n",
    "#     \"model__subsample\": [0.5, 0.6],\n",
    "#     # More conservative algorithm\n",
    "#     \"model__min_split_loss\": [1, 10],\n",
    "#     \"model__min_child_weight\": [1, 10],\n",
    "#     \"model__max_delta_step\": [3, 9],\n",
    "#     \"model__lambda\": [1, 10],\n",
    "#     \"model__alpha\": [1, 10],\n",
    "#     # Other\n",
    "#     # \"max_leaves\": [],\n",
    "# }]\n",
    "# # This crashes my laptop\n",
    "# xgb_params = [{\n",
    "#     # Prevents overfitting\n",
    "#     \"model__learning_rate\": [0.5, 0.7, 0.9],\n",
    "#     \"model__max_depth\": [5, 10,15],\n",
    "#     \"model__subsample\": [0.4, 0.6, 0.8],\n",
    "#     # More conservative algorithm\n",
    "#     \"model__min_split_loss\": [0.1, 1, 10],\n",
    "#     \"model__min_child_weight\": [0.1, 1, 10],\n",
    "#     \"model__max_delta_step\": [3, 6, 9],\n",
    "#     \"model__lambda\": [0.1, 1, 10],\n",
    "#     \"model__alpha\": [0.1, 1, 10],\n",
    "#     # Other\n",
    "#     # \"max_leaves\": [],\n",
    "# }]\n",
    "# # Overnight: ~6.25 hours\n",
    "# xgb_params = [{\n",
    "#     # Prevents overfitting\n",
    "#     \"model__learning_rate\": [0.3, 0.5, 0.7, 0.8, 0.9],\n",
    "#     \"model__max_depth\": [5, 10, 15, 20, 25, 30],\n",
    "#     \"model__subsample\": [0.2, 0.4, 0.6, 0.8],\n",
    "#     # More conservative algorithm\n",
    "#     \"model__min_split_loss\": [0.01, 0.1, 1, 10, 100],\n",
    "#     \"model__min_child_weight\": [0.01, 0.1, 1, 10, 100],\n",
    "#     \"model__max_delta_step\": [3, 6, 9],\n",
    "#     \"model__lambda\": [0.01, 0.1, 1, 10, 100],\n",
    "#     \"model__alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "#     # Other\n",
    "#     # \"max_leaves\": [],\n",
    "# }]\n",
    "\n",
    "def calculate_runtime(params, seconds_per_model=0.25, cv_folds=5):\n",
    "    \"\"\"\n",
    "    For reference:\n",
    "    Every 36,000 permutations => 1 hour at 0.1 secs/model\n",
    "    5 hours at 0.1 secs/model is 180,000 total permutations\n",
    "    6 hours at 0.1 secs/model is 216,000 total permutations\n",
    "    7 hours at 0.1 secs/model is 252,000 total permutations\n",
    "    8 hours at 0.1 secs/model is 288,000 total permutations\n",
    "    \"\"\"\n",
    "    params = params[0]\n",
    "    total_permutations = 1\n",
    "    for param, lst in params.items():\n",
    "        total_permutations *= len(lst)\n",
    "    \n",
    "    total_permutations *= cv_folds\n",
    "    time = ((total_permutations * seconds_per_model) / 60)\n",
    "\n",
    "    return total_permutations, time\n",
    "\n",
    "rf_permutations, rf_runtime = calculate_runtime(rf_params)\n",
    "xgb_permutations, xgb_runtime = calculate_runtime(xgb_params)\n",
    "print(f\"RF - GridSearchCV has {rf_permutations} permutations and will take {rf_runtime:.2f} minutes (={rf_runtime/60:.2f})\")\n",
    "print(f\"XGB - GridSearchCV has {xgb_permutations} permutations and will take {xgb_runtime:.2f} minutes (={xgb_runtime/60:.2f} hours)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = GridSearchCV(rf_pipeline,\n",
    "                      param_grid=rf_params,\n",
    "                      scoring='f1',\n",
    "                      cv=5,\n",
    "                      verbose=2)\n",
    "xgb = GridSearchCV(xgb_pipeline,\n",
    "                      param_grid=xgb_params,\n",
    "                      scoring='f1',\n",
    "                      cv=5,\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pipeline.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.best_score_)\n",
    "print(rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_pipeline.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_temp = numpy.asarray(X_train).astype(numpy.float32)\n",
    "# nn_pipeline.fit(\n",
    "#     X_temp, y_train,\n",
    "#     model__validation_data=(X_test, y_test),\n",
    "#     model__batch_size=64,\n",
    "#     model__epochs=1000,\n",
    "#     model__callbacks=[early_stopping],\n",
    "#     # model__verbose=0, # hide the output because we have so many epochs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_predict_train = rf_pipeline.predict(X_train)\n",
    "# rf_predict_test = rf_pipeline.predict(X_test)\n",
    "rf_predict_train = rf.predict(X_train)\n",
    "rf_predict_test = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_f1_score = f1_score(y_train, rf_predict_train, average=\"binary\")\n",
    "print(f\"The F1 Score for the RF model on the training set is: {rf_train_f1_score * 100:.2f}%\")\n",
    "rf_test_f1_score = f1_score(y_test, rf_predict_test, average=\"binary\")\n",
    "print(f\"The F1 Score for the RF model on the test set  is: {rf_test_f1_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_predict_train = xgb_pipeline.predict(X_train)\n",
    "# xgb_predict_test = xgb_pipeline.predict(X_test)\n",
    "xgb_predict_train = xgb.predict(X_train)\n",
    "xgb_predict_test = xgb.predict(X_test)\n",
    "print(xgb.best_score_)\n",
    "print(xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_f1_score = f1_score(y_train, xgb_predict_train, average=\"binary\")\n",
    "print(f\"The F1 Score for the XGB model on the training set is: {xgb_train_f1_score * 100:.2f}%\")\n",
    "xgb_test_f1_score = f1_score(y_test, xgb_predict_test, average=\"binary\")\n",
    "print(f\"The F1 Score for the XGB model on the test set  is: {xgb_test_f1_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# rf_file = \"rf.pkl\"\n",
    "# rf.estimator[0].transformers[0][1]\n",
    "# rf = joblib.load(rf_file)\n",
    "# rf_predict_train = rf.predict(X_train)\n",
    "# rf_predict_test = rf.predict(X_test)\n",
    "# rf_train_f1_score = f1_score(y_train, rf_predict_train, average=\"binary\")\n",
    "# rf_test_f1_score = f1_score(y_test, rf_predict_test, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = compute_class_weight('balanced', classes=[1,0], y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import save_model\n",
    "importlib.reload(save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prove that undersampling is helping\n",
    "# the std should be very small (comparing with and without undersampling)\n",
    "\n",
    "# cv_results = rf.cv_results_[\"split0_test_score\"].mean()\n",
    "cv_results = rf.cv_results_\n",
    "num_folds = 5\n",
    "for x in range(num_folds):\n",
    "    curr = f\"split{x}_test_score\"\n",
    "    print(\n",
    "        f\"F1 mean +/- std. dev.: for split={curr}: \"\n",
    "        f\"{cv_results[curr].mean():.3f} +/- \"\n",
    "        f\"{cv_results[curr].std():.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model.save_model(rf, \"rf_4.pkl\", X_train, rf_params[0], rf_test_f1_score, description=\"[supposedly] using undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model.save_model(xgb, \"xgb_3.pkl\", X_train, xgb_params[0], xgb_test_f1_score, description=\"using undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pipeline[0] is the preprocessor object, which has the function get_feature_names_out\n",
    "# encoded_features = rf_pipeline[0].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell did not work initially. VSCode might need to reboot to recognize the newly downloaded graphviz\n",
    "# estimator = rf_pipeline[1].estimators_[5]\n",
    "\n",
    "# from sklearn.tree import export_graphviz\n",
    "# # Export as dot file\n",
    "# export_graphviz(estimator, out_file='tree.dot', \n",
    "#                 feature_names = encoded_features,\n",
    "#                 class_names = [\"Win\", \"Loss\"],\n",
    "#                 rounded = True, proportion = False, \n",
    "#                 precision = 2, filled = True)\n",
    "\n",
    "# # Convert to png using system command (requires Graphviz)\n",
    "# from subprocess import call\n",
    "# call(['dot', '-Tpng', 'C:\\\\Users\\\\Beau\\\\Desktop\\\\ML\\\\faker\\\\tree.dot', '-o', 'C:\\\\Users\\\\Beau\\\\Desktop\\\\ML\\\\faker\\\\tree.png', '-Gdpi=600'])\n",
    "\n",
    "# # Display in jupyter notebook\n",
    "# from IPython.display import Image\n",
    "# Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a PNG file for one of the trees\n",
    "# from sklearn import tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# fn=encoded_features\n",
    "# cn=[\"Win\", \"Loss\"]\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "# tree.plot_tree(rf.estimator[1].estimators_[0],\n",
    "# # tree.plot_tree(rf_pipeline[1].estimators_[0],\n",
    "#                feature_names = fn, \n",
    "#                class_names=cn,\n",
    "#                filled = True);\n",
    "# fig.savefig('rf_individualtree.png')\n",
    "# print(\"Generated one tree\")  # takes about 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce overfitting,\n",
    "A. Add more examples\n",
    "B. Reduce number of features\n",
    "C. Increase regularization parameter\n",
    "D. Trees - reduce depth and other parameters\n",
    "    - early_stopping_rounds: can use high n_estimators, and then modify this parameter. note: this is a parameter for XGBClassifier.fit()\n",
    "    - learning_rate: large learning_rate+large n_estimators leads to more accurate models, but takes longer\n",
    "\n",
    "Error Analysis\n",
    "    - manually examine 100 examples and see if there is a pattern\n",
    "\n",
    "Iterative Loop of ML\n",
    "    Choose Architecture\n",
    "    Train\n",
    "    Diagnostics (bias, variance, error analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Performance    10.6%           10.6%       10.6%\n",
    "                            +0.2%           +4.4%       +4.4%\n",
    "Training Error          10.8%           15.0%       15.0%\n",
    "                            +4.0%           +0.5%       +4.7%\n",
    "CV Error                14.8%           15.5%       19.7%\n",
    "                        (High Variance) (High Bias) (High Variance & Bias)\n",
    "\n",
    "No Hyperparameter Tuning\n",
    "RF\n",
    "Baseline Performance    35.0%\n",
    "                            -35.0%\n",
    "Training Error          0.00%\n",
    "                            +22.9%\n",
    "CV Error                22.9%\n",
    "                        (High Variance)\n",
    "XGB\n",
    "Baseline Performance    35.0%\n",
    "                            -30.8%\n",
    "Training Error          4.20%\n",
    "                            +23.4%\n",
    "CV Error                27.6%\n",
    "                        (High Variance)\n",
    "\n",
    "After Hyperparameter Tuning\n",
    "RF\n",
    "Baseline Performance    35.0%\n",
    "                            -16.0%\n",
    "Training Error          19.0%\n",
    "                            +2.10%\n",
    "CV Error                21.1%\n",
    "                        (High Variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Parameters\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "\"Prevents overfitting\" \\\n",
    "`n_estimators`. default=100. increase => overfitting \\\n",
    "`max_depth`. default=None. increase => overfitting \\\n",
    "`max_samples`. similar to `subsample` for XGB?\n",
    "\n",
    "Other \\\n",
    "`max_leaf_nodes`. default=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB Parameters\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n",
    "\"Prevents overfitting\" \\\n",
    "`eta` (aka `learning_rate`). default=0.3 \\\n",
    "`max_depth`. default=6. range=[0, infinity]. larger max_depth => overfitting. \\\n",
    "`subsample`. default=1. range=(0,1]\n",
    "\n",
    "\"More conversative\" \\\n",
    "`gamma` (aka `min_split_loss`). default=0.0. range=[0, infinity]. larger gamma => conservative algorithm \\\n",
    "`min_child_weight`. default=1. range=[0, infinity]. increase => conservative \\\n",
    "`max_delta_step`. default=0. range=[0, infinity]. Set to 1-10. \\\n",
    "`lambda` (aka `reg_lambda`). default=1. increase => conservative \\\n",
    "`alpha` (aka `reg_alpha`). default=0. increase => conservative\n",
    "\n",
    "Other \\\n",
    "`max_leaves`. default=0\n",
    "\n",
    "Useful Code \\\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_intro.html#setting-parameters\n",
    "```\n",
    "import xgboost as xgb\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "bst.save_model('0001.model')\n",
    "bst.dump_model('dump.raw.txt')\n",
    "bst.dump_model('dump.raw.txt', 'featmap.txt')\n",
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model('model.bin')  # load data\n",
    "\n",
    "# Plotting\n",
    "xgb.plot_importance(bst)   \n",
    "xgb.plot_tree(bst, num_trees=2)\n",
    "xgb.to_graphviz(bst, num_trees=2)\n",
    "```\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(xgb_model, {'max_depth': [2, 4, 6],\n",
    "                                   'n_estimators': [50, 100, 200]}, verbose=1,\n",
    "                       n_jobs=2)\n",
    "clf.fit(X, y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "```\n",
    "\n",
    "```\n",
    "# Cross-Validation with XGB\n",
    "# https://xgboost.readthedocs.io/en/stable/python/examples/cross_validation.html#sphx-glr-python-examples-cross-validation-py\n",
    "xgb.cv(param, dtrain, num_round, nfold=5,\n",
    "       metrics={'error'}, seed=0,\n",
    "       callbacks=[xgb.callback.EvaluationMonitor(show_stdv=True)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    # seaborn.histplot(data=X_train, x=col)\n",
    "    seaborn.histplot(data=X_train, x=col, kde=True)\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    seaborn.barplot(data=X_train, x=col, y=y_train)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pandas_no_truncate()\n",
    "for col in categorical_cols:\n",
    "    print(X_train.pivot_table(index=y_train, columns=col, aggfunc=\"size\", fill_value=0))\n",
    "print_pandas_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "xgb = joblib.load(\"C:\\\\Users\\\\Beau\\\\Desktop\\\\ML\\\\faker\\\\models\\\\rf_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "display.plot()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_columns', None)\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "count = 0\n",
    "for i, match in X_test.iterrows():\n",
    "    match = pandas.DataFrame(match).T  # convert 'match':Series to DataFrame (which will have shape (13,1)) and transpose to have shape (1,13) as a compatible datatype for *.predict \n",
    "    prediction = rf.predict(match)[0]\n",
    "\n",
    "    if prediction == y_test.loc[i]:\n",
    "        continue\n",
    "    # Type 1 - False Positive\n",
    "    if prediction == 1 and y_test.loc[i] != prediction:\n",
    "        print(\"TYPE 1 ERROR\")\n",
    "    # Type 2 - False Negative\n",
    "    if prediction == 0 and y_test.loc[i] != prediction:\n",
    "        print(\"TYPE 2 ERROR\")\n",
    "    print(match)\n",
    "    print(dataframe.loc[i])\n",
    "    count += 1\n",
    "    if count == 15:\n",
    "        break\n",
    "pandas.reset_option('display.max_columns')\n",
    "pandas.reset_option('display.max_colwidth')\n",
    "pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_row(row):\n",
    "    \"\"\"Print full row details. Resets panda options\"\"\"\n",
    "    # Removes truncations\n",
    "    pandas.set_option('display.max_columns', None)\n",
    "    pandas.set_option('display.max_colwidth', None)\n",
    "    pandas.set_option('display.max_rows', None)\n",
    "\n",
    "    # Print\n",
    "    print(row)\n",
    "\n",
    "    # Reset\n",
    "    pandas.reset_option('display.max_columns')\n",
    "    pandas.reset_option('display.max_colwidth')\n",
    "    pandas.reset_option('display.max_rows')\n",
    "\n",
    "def print_pandas_no_truncate():\n",
    "    pandas.set_option('display.max_columns', None)\n",
    "    pandas.set_option('display.max_colwidth', None)\n",
    "    pandas.set_option('display.max_rows', None)\n",
    "\n",
    "def print_pandas_reset():\n",
    "    pandas.reset_option('display.max_columns')\n",
    "    pandas.reset_option('display.max_colwidth')\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1920d7ec77403009ba0308836b7e203e7c3913f8a56778c2faa31a2088572210"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
